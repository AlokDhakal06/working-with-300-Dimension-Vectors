{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "dataset = pd.read_csv('train_features.csv',encoding = \"ISO-8859-1\",nrows=50000)\n",
    "X = dataset.iloc[:, [5,10,11,12,13,14,15,16,17,20,21,23,24,25,26]].values\n",
    "y = dataset.iloc[:, 31].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking care of missing data\n",
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(missing_values = 'NaN', strategy = 'mean', axis = 0)\n",
    "imputer = imputer.fit(X)\n",
    "X = imputer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hk\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter question 1:hi\n",
      "enter question 2:hey\n"
     ]
    }
   ],
   "source": [
    "question1=input(\"enter question 1:\")\n",
    "question2=input(\"enter question 2:\")\n",
    "#calculation of features from the input question\n",
    "len_1=len(question1)\n",
    "len_2=len(question2)\n",
    "diff_len=len_1-len_2\n",
    "len_char_q1=len(''.join(set(str(question1).replace(' ', ''))))\n",
    "len_char_q2=len(''.join(set(str(question2).replace(' ', ''))))\n",
    "len_word_q1=len(str(question1).split())\n",
    "len_word_q2=len(str(question2).split())\n",
    "common_words=len(set(str(question1).lower().split()).intersection(set(str(question2).lower().split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hk\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:35: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "#fuzzy\n",
    "from fuzzywuzzy import fuzz\n",
    "fuzz_qratio=fuzz.QRatio(str(question1), str(question2))\n",
    "fuzz_WRatio=fuzz.WRatio(str(question1), str(question2))\n",
    "fuzz_partial_ratio=fuzz.partial_ratio(str(question1), str(question2))\n",
    "fuzz_partial_token_set_ratio=fuzz.partial_token_set_ratio(str(question1), str(question2))\n",
    "fuzz_partial_token_sort_ratio=fuzz.partial_token_sort_ratio(str(question1), str(question2))\n",
    "fuzz_token_set_ratio=fuzz.token_set_ratio(str(question1), str(question2))\n",
    "fuzz_token_sort_ratio=fuzz.token_sort_ratio(str(question1), str(question2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sen2vec\n",
    "import scipy\n",
    "question1_vectors = scipy.sparse.lil_matrix((dataset.shape[0], 300))\n",
    "question2_vectors = scipy.sparse.lil_matrix((dataset.shape[0], 300))\n",
    "error_count = 0\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "def sent2vec(s):\n",
    "    words = str(s).lower() # .decode('utf-8') ahile kaam lagdaina\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(model[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    return v / np.sqrt((v ** 2).sum())\n",
    "question1_vector=sent2vec(question1)\n",
    "question2_vector=sent2vec(question2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cosine distances\n",
    "from scipy.spatial.distance import cosine, cityblock, jaccard, canberra, euclidean, minkowski, braycurtis\n",
    "cosine_distance= cosine(question1_vector,question2_vector)\n",
    "cityblock_distance=cityblock(question1_vector,question2_vector)\n",
    "jaccard_distance=jaccard(question1_vector,question2_vector)\n",
    "canberra_distance=canberra(question1_vector,question2_vector)\n",
    "euclidean_distance=euclidean(question1_vector,question2_vector)\n",
    "minkowski_distance=minkowski(question1_vector,question2_vector,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#ANN\n",
    "#importing libraries keras\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 10s 249us/step - loss: 0.5360 - acc: 0.6820\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 5s 136us/step - loss: 0.5266 - acc: 0.6888\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 5s 137us/step - loss: 0.5244 - acc: 0.6905\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 6s 140us/step - loss: 0.5220 - acc: 0.6978\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 5s 118us/step - loss: 0.5209 - acc: 0.7025\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - 5s 117us/step - loss: 0.5184 - acc: 0.7068\n",
      "Epoch 7/20\n",
      "40000/40000 [==============================] - 5s 114us/step - loss: 0.5143 - acc: 0.7115\n",
      "Epoch 8/20\n",
      "40000/40000 [==============================] - 5s 115us/step - loss: 0.5114 - acc: 0.7134\n",
      "Epoch 9/20\n",
      "40000/40000 [==============================] - 6s 143us/step - loss: 0.5085 - acc: 0.7153\n",
      "Epoch 10/20\n",
      "40000/40000 [==============================] - 6s 139us/step - loss: 0.5051 - acc: 0.7205\n",
      "Epoch 11/20\n",
      "40000/40000 [==============================] - 5s 137us/step - loss: 0.5020 - acc: 0.7229\n",
      "Epoch 12/20\n",
      "40000/40000 [==============================] - 5s 132us/step - loss: 0.5006 - acc: 0.7222\n",
      "Epoch 13/20\n",
      "40000/40000 [==============================] - 5s 134us/step - loss: 0.4997 - acc: 0.7237\n",
      "Epoch 14/20\n",
      "40000/40000 [==============================] - 6s 145us/step - loss: 0.4993 - acc: 0.7241\n",
      "Epoch 15/20\n",
      "40000/40000 [==============================] - 6s 138us/step - loss: 0.4984 - acc: 0.7221\n",
      "Epoch 16/20\n",
      "40000/40000 [==============================] - 5s 134us/step - loss: 0.4982 - acc: 0.7262\n",
      "Epoch 17/20\n",
      "40000/40000 [==============================] - 5s 134us/step - loss: 0.4982 - acc: 0.7241\n",
      "Epoch 18/20\n",
      "40000/40000 [==============================] - 6s 142us/step - loss: 0.4977 - acc: 0.7247\n",
      "Epoch 19/20\n",
      "40000/40000 [==============================] - 6s 138us/step - loss: 0.4975 - acc: 0.7251\n",
      "Epoch 20/20\n",
      "40000/40000 [==============================] - 6s 143us/step - loss: 0.4971 - acc: 0.7249\n"
     ]
    }
   ],
   "source": [
    "#initializing ANN\n",
    "classifier=Sequential()\n",
    "#adding input layer and first hidden layer \n",
    "classifier.add(Dense(input_dim=15, kernel_initializer=\"uniform\", units=8, activation=\"relu\"))\n",
    "#addinh second hidden layer\n",
    "classifier.add(Dense( kernel_initializer=\"uniform\", units=8, activation=\"relu\"))\n",
    "#adding output layer\n",
    "classifier.add(Dense(kernel_initializer=\"uniform\", units=1, activation=\"sigmoid\"))\n",
    "#compiling ANN\n",
    "classifier.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "#fitting ANN to training set\n",
    "classifier.fit(X_train,y_train,batch_size=10,epochs=20)\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred=(y_pred>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "classifier.save(\"ANNmodel_ip.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
